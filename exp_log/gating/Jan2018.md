#### 2018/01/14

**sigyn**
* squeezenet_v11: accuracy is too low
  - display
  ```
  ⠇ epoch: 40.17 | gate.loss: 2.54961±3.58% | gate: 81.98% | gate.loss: 2.54961±3.58% | gate: 81.98% | accuracy: 45.60% | loss:
  ```
  - command
  ```
  LD_PRELOAD=/usr/lib/libtcmalloc.so ./my datasets/imagenet.yaml trainers/squeezenet.yaml models/gate/output/squeezenet_v11.yaml system.checkpoint.load=latest system.num_gpus=2 system.visible_gpus=[0,1] system.batch_size_per_gpu=256 train.learning_rate._initial=0.001 system.preprocess.num_threads=32 train.learning_rate.decay_steps=10 train
  ```
  - yaml
  ```
_import: ../../squeezenet_v11.yaml
model:
    name: squeezenet_v11
    layers:
        _gated_conv: &gated_conv
            type: gated_convolution
            density: 0.8
            trainable: false
            online: false
            weight: 0.00001
        _fire: &fire
            kwargs: {should_gate: null, trainable: null}
            layers:
                expand1: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
                expand3: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
        #conv1: {<<: *gated_conv, should_gate: false, trainable: true}
        #fire2: {<<: *fire, should_gate: false, trainable: true}
        #fire3: {<<: *fire, should_gate: false, trainable: true}
        #fire4: {<<: *fire, should_gate: false, trainable: true}
        #fire5: {<<: *fire, should_gate: false, trainable: true}
        fire6: {<<: *fire, should_gate: true, trainable: true}
        fire7: {<<: *fire, should_gate: true, trainable: true}
        fire8: {<<: *fire, should_gate: true, trainable: true}
        fire9: {<<: *fire, should_gate: true, trainable: true}
  ```

* mobilenet: accurracy is too low
  - display
  ```
  ⠙ epoch: 2.17 | gate.loss: 1.74995±7.01% | gate: 83.42% | gate.loss: 1.74995±7.01% | gate: 83.42% | accuracy: 59.61% | loss:   1
  ```
  - command
  ```
  LD_PRELOAD=/usr/lib/libtcmalloc.so ./my datasets/imagenet.yaml trainers/mobilenet.yaml models/mobilenet.yaml models/gate/output/mobilenet.yaml system.checkpoint.load=latest system.num_gpus=2 system.visible_gpus=[2,3] system.batch_size_per_gpu=128 train.learning_rate._initial=0.001 system.preprocess.num_threads=32 train.learning_rate.decay_steps=10 reset-num-epochs train
  ```
  - yaml
  ```
  _import: ../../mobilenet.yaml
  model:
      name: mobilenet_v1
      layers:
          _gated_conv: &gated_conv
              type: gated_convolution
              density: 0.8
              online: false
              trainable: false
              weight: 0.00001
          _dsconv: &dsconv
              kwargs: {should_gate: null, trainable: null}
              layers:
                  pointwise: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
          #conv0: {<<: *gated_conv, should_gate: false}
          #conv1: {<<: *dsconv, should_gate: false}
          #conv2: {<<: *dsconv, should_gate: false}
          #conv3: {<<: *dsconv, should_gate: false}
          #conv4: {<<: *dsconv, should_gate: false}
          #conv5: {<<: *dsconv, should_gate: false}
          #conv6: {<<: *dsconv, should_gate: false}
          #conv7: {<<: *dsconv, should_gate: false}
          #conv8: {<<: *dsconv, should_gate: false}
          conv9: {<<: *dsconv, should_gate: true, trainbale: true}
          conv10: {<<: *dsconv, should_gate: true, trainable: true}
          conv11: {<<: *dsconv, should_gate: true, trainable: true}
          conv12: {<<: *dsconv, should_gate: true, trainable: true}
          conv13: {<<: *dsconv, should_gate: true, trainable: true}
  ```

**heimdall**
* squeezenet_v11: accuracy is not reaching 50%+
  - display
  ```
  ⠼ epoch: 42.78 | gate.loss: 2.35076±6.25% | gate: 80.70% | gate.loss: 2.35076±6.25% | gate: 80.70% | accuracy: 48.93% | loss:
  ```
  - command
  ```
  LD_PRELOAD=/usr/lib/libtcmalloc.so.4 ./my datasets/imagenet.yaml trainers/alexnet.yaml models/gate/output/squeezenet_v11.yaml system.checkpoint.load=latest system.batch_size_per_gpu=128 system.preprocess.num_threads=12 system.learning_rate._initial=0.001 system.num_gpus=2 system.visible_gpus=[0,1] train.learning_rate.decay_steps=10 train
  ```
  - yaml
  ```
  --
  _import: ../../squeezenet_v11.yaml
  model:
      name: squeezenet_v11
      layers:
          _gated_conv: &gated_conv
              type: gated_convolution
              density: 0.8
              trainable: false
              online: true
              weight: 0.00001
          _fire: &fire
              kwargs: {should_gate: null, trainable: null}
              layers:
                  expand1: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
                  expand3: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
          #conv1: {<<: *gated_conv, should_gate: false, trainable: false}
          #fire2: {<<: *fire, should_gate: false, trainable: false}
          #fire3: {<<: *fire, should_gate: false, trainable: false}
          #fire4: {<<: *fire, should_gate: false, trainable: false}
          #fire5: {<<: *fire, should_gate: false, trainable: false}
          #fire6: {<<: *fire, should_gate: false, trainable: false}
          #fire7: {<<: *fire, should_gate: false, trainable: false}
          #fire8: {<<: *fire, should_gate: false, trainable: false}
          fire9: {<<: *fire, should_gate: true, trainable: true}
  ```

#### 2018/01/15
**heimdall**
* squeezenet_v11: gate all layers with 95%, this does not decrease the gate percentage, then tried to gate all layers with 90%, which works.
* After a few epochs with all layers set to non-trainable the gate density can decrease to around 95% with an accuracy of around 45%. I now set all layers to trainable to try reach back to around 50%.

```
⠧ epoch: 25.07 | gate.loss: 2.58417±6.11% | gate: 96.22% | gate.loss: 2.58417±6.11% | gate: 96.22% | accuracy: 48.10% | loss:
```

```
LD_PRELOAD=/usr/lib/libtcmalloc.so.4 ./my datasets/imagenet.yaml trainers/alexnet.yaml models/gate/output/squeezenet_v11.yaml system.checkpoint.load=retrained system.batch_size_per_gpu=128 system.preprocess.num_threads=12 system.learning_rate._initial=0.001 system.num_gpus=2 system.visible_gpus=[0,1] train.learning_rate.decay_steps=30 train
```

```
---
_import: ../../squeezenet_v11.yaml
model:
    name: squeezenet_v11
    layers:
        _gated_conv: &gated_conv
            type: gated_convolution
            density: 0.90
            trainable: false
            online: true
            weight: 0.0001
        _fire: &fire
            kwargs: {should_gate: null, trainable: null}
            layers:
                squeeze: {trainable: ^(trainable)}
                expand1: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
                expand3: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
        conv1: {<<: *gated_conv, should_gate: true, trainable: false}
        fire2: {<<: *fire, should_gate: true, trainable: false}
        fire3: {<<: *fire, should_gate: true, trainable: false}
        fire4: {<<: *fire, should_gate: true, trainable: false}
        fire5: {<<: *fire, should_gate: true, trainable: false}
        fire6: {<<: *fire, should_gate: true, trainable: false}
        fire7: {<<: *fire, should_gate: true, trainable: false}
        fire8: {<<: *fire, should_gate: true, trainable: false}
        fire9: {<<: *fire, should_gate: true, trainable: false}
```

**sigyn**

* squeezenet_v11: gate all layers with 90%, started this with every layer set to be non-trainable. The previous run is saved as
`gate_offline_80_fire6_90_epoch.data-00000-of-00001`.
* The trick for offline to quickly reach its required gating density is to set `weight` to a relatively large value. However, a super-large weight can cause the gate loss to diverge.

```
LD_PRELOAD=/usr/lib/libtcmalloc.so ./my datasets/imagenet.yaml trainers/squeezenet.yaml models/gate/output/squeezenet_v11.yaml system.checkpoint.load=retrained system.num_gpus=2 system.visible_gpus=[0,1] system.batch_size_per_gpu=256 train.learning_rate._initial=0.001 system.preprocess.num_threads=32 train.learning_rate.decay_steps=20 reset-num-epochs train
```

#### 2018/01/17
**Summary**
1. Depreciate offline training, and work on experiemtns for online training
2. The training method is to set every layer to non-trainable but should gate to
true. This helps to reach the required density. The next step is to lower the
gating `weight` and set every layer to trainable to recover the accuracy.
3. `fc` layers do not require gating because inputs are flattened.

|  network  |  density  | top1 acc | top5 acc |           checkpoint          |  server  |
| --------- | --------- |----------|----------| ------------------------------| -------- |
|  squeeze  |   98.07%  |  56.51   |  79.39   |  gate_90_online_trainable2    |   sigyn  |
|  squeeze  |   95.71%  |  55.72   |  78.88   |  gate_90_online_trainable     |   sigyn  |
|  mobile   |   98.3%   |  68.75   |  88.34   |  gate_90_online_trainable     |   sigyn  |

**heimdall**
* Trained a 90%(set density) for around 13 epochs.
This is saved as checkpoint `gate_90_online_trainable2`.

```
⣿ validate: 100.00% | gate.loss: 0.00000±nan% | gate: 98.07% | top1: 56.51% | top5: 79.39% | tp:  423/s                                           
- Evaluation complete.
-     top1: 56.51%, top5: 79.39% [50000 images]
```
* There is another checkpoint `gate_90_online_trainable`.

```
⣿ validate: 100.00% | gate.loss: 1.69347±2.35% | gate: 95.71% | top1: 55.72% | top5: 78.88% | tp:  409/s                                          
- Evaluation complete.
-     top1: 55.72%, top5: 78.88% [50000 images]
```

**sigyn**
sigyn is down for moving location.
The following commands were running:
