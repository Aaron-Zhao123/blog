#### 2018/01/14

**sigyn**
* squeezenet_v11: accuracy is too low
  - display
  ```
  ⠇ epoch: 40.17 | gate.loss: 2.54961±3.58% | gate: 81.98% | gate.loss: 2.54961±3.58% | gate: 81.98% | accuracy: 45.60% | loss:
  ```
  - command
  ```
  LD_PRELOAD=/usr/lib/libtcmalloc.so ./my datasets/imagenet.yaml trainers/squeezenet.yaml models/gate/output/squeezenet_v11.yaml system.checkpoint.load=latest system.num_gpus=2 system.visible_gpus=[0,1] system.batch_size_per_gpu=256 train.learning_rate._initial=0.001 system.preprocess.num_threads=32 train.learning_rate.decay_steps=10 train
  ```
  - yaml
  ```
_import: ../../squeezenet_v11.yaml
model:
    name: squeezenet_v11
    layers:
        _gated_conv: &gated_conv
            type: gated_convolution
            density: 0.8
            trainable: false
            online: false
            weight: 0.00001
        _fire: &fire
            kwargs: {should_gate: null, trainable: null}
            layers:
                expand1: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
                expand3: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
        #conv1: {<<: *gated_conv, should_gate: false, trainable: true}
        #fire2: {<<: *fire, should_gate: false, trainable: true}
        #fire3: {<<: *fire, should_gate: false, trainable: true}
        #fire4: {<<: *fire, should_gate: false, trainable: true}
        #fire5: {<<: *fire, should_gate: false, trainable: true}
        fire6: {<<: *fire, should_gate: true, trainable: true}
        fire7: {<<: *fire, should_gate: true, trainable: true}
        fire8: {<<: *fire, should_gate: true, trainable: true}
        fire9: {<<: *fire, should_gate: true, trainable: true}
  ```

* mobilenet: accurracy is too low
  - display
  ```
  ⠙ epoch: 2.17 | gate.loss: 1.74995±7.01% | gate: 83.42% | gate.loss: 1.74995±7.01% | gate: 83.42% | accuracy: 59.61% | loss:   1
  ```
  - command
  ```
  LD_PRELOAD=/usr/lib/libtcmalloc.so ./my datasets/imagenet.yaml trainers/mobilenet.yaml models/mobilenet.yaml models/gate/output/mobilenet.yaml system.checkpoint.load=latest system.num_gpus=2 system.visible_gpus=[2,3] system.batch_size_per_gpu=128 train.learning_rate._initial=0.001 system.preprocess.num_threads=32 train.learning_rate.decay_steps=10 reset-num-epochs train
  ```
  - yaml
  ```
  _import: ../../mobilenet.yaml
  model:
      name: mobilenet_v1
      layers:
          _gated_conv: &gated_conv
              type: gated_convolution
              density: 0.8
              online: false
              trainable: false
              weight: 0.00001
          _dsconv: &dsconv
              kwargs: {should_gate: null, trainable: null}
              layers:
                  pointwise: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
          #conv0: {<<: *gated_conv, should_gate: false}
          #conv1: {<<: *dsconv, should_gate: false}
          #conv2: {<<: *dsconv, should_gate: false}
          #conv3: {<<: *dsconv, should_gate: false}
          #conv4: {<<: *dsconv, should_gate: false}
          #conv5: {<<: *dsconv, should_gate: false}
          #conv6: {<<: *dsconv, should_gate: false}
          #conv7: {<<: *dsconv, should_gate: false}
          #conv8: {<<: *dsconv, should_gate: false}
          conv9: {<<: *dsconv, should_gate: true, trainbale: true}
          conv10: {<<: *dsconv, should_gate: true, trainable: true}
          conv11: {<<: *dsconv, should_gate: true, trainable: true}
          conv12: {<<: *dsconv, should_gate: true, trainable: true}
          conv13: {<<: *dsconv, should_gate: true, trainable: true}
  ```

**heimdall**
* squeezenet_v11: accuracy is not reaching 50%+
  - display
  ```
  ⠼ epoch: 42.78 | gate.loss: 2.35076±6.25% | gate: 80.70% | gate.loss: 2.35076±6.25% | gate: 80.70% | accuracy: 48.93% | loss:
  ```
  - command
  ```
  LD_PRELOAD=/usr/lib/libtcmalloc.so.4 ./my datasets/imagenet.yaml trainers/alexnet.yaml models/gate/output/squeezenet_v11.yaml system.checkpoint.load=latest system.batch_size_per_gpu=128 system.preprocess.num_threads=12 system.learning_rate._initial=0.001 system.num_gpus=2 system.visible_gpus=[0,1] train.learning_rate.decay_steps=10 train
  ```
  - yaml
  ```
  --
  _import: ../../squeezenet_v11.yaml
  model:
      name: squeezenet_v11
      layers:
          _gated_conv: &gated_conv
              type: gated_convolution
              density: 0.8
              trainable: false
              online: true
              weight: 0.00001
          _fire: &fire
              kwargs: {should_gate: null, trainable: null}
              layers:
                  expand1: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
                  expand3: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
          #conv1: {<<: *gated_conv, should_gate: false, trainable: false}
          #fire2: {<<: *fire, should_gate: false, trainable: false}
          #fire3: {<<: *fire, should_gate: false, trainable: false}
          #fire4: {<<: *fire, should_gate: false, trainable: false}
          #fire5: {<<: *fire, should_gate: false, trainable: false}
          #fire6: {<<: *fire, should_gate: false, trainable: false}
          #fire7: {<<: *fire, should_gate: false, trainable: false}
          #fire8: {<<: *fire, should_gate: false, trainable: false}
          fire9: {<<: *fire, should_gate: true, trainable: true}
  ```

#### 2018/01/15
**heimdall**
* squeezenet_v11: gate all layers with 95%, this does not decrease the gate percentage, then tried to gate all layers with 90%, which works.
* After a few epochs with all layers set to non-trainable the gate density can decrease to around 95% with an accuracy of around 45%. I now set all layers to trainable to try reach back to around 50%.

```
⠧ epoch: 25.07 | gate.loss: 2.58417±6.11% | gate: 96.22% | gate.loss: 2.58417±6.11% | gate: 96.22% | accuracy: 48.10% | loss:
```

```
LD_PRELOAD=/usr/lib/libtcmalloc.so.4 ./my datasets/imagenet.yaml trainers/alexnet.yaml models/gate/output/squeezenet_v11.yaml system.checkpoint.load=retrained system.batch_size_per_gpu=128 system.preprocess.num_threads=12 system.learning_rate._initial=0.001 system.num_gpus=2 system.visible_gpus=[0,1] train.learning_rate.decay_steps=30 train
```

```
---
_import: ../../squeezenet_v11.yaml
model:
    name: squeezenet_v11
    layers:
        _gated_conv: &gated_conv
            type: gated_convolution
            density: 0.90
            trainable: false
            online: true
            weight: 0.0001
        _fire: &fire
            kwargs: {should_gate: null, trainable: null}
            layers:
                squeeze: {trainable: ^(trainable)}
                expand1: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
                expand3: {<<: *gated_conv, should_gate: ^(should_gate), trainable: ^(trainable)}
        conv1: {<<: *gated_conv, should_gate: true, trainable: false}
        fire2: {<<: *fire, should_gate: true, trainable: false}
        fire3: {<<: *fire, should_gate: true, trainable: false}
        fire4: {<<: *fire, should_gate: true, trainable: false}
        fire5: {<<: *fire, should_gate: true, trainable: false}
        fire6: {<<: *fire, should_gate: true, trainable: false}
        fire7: {<<: *fire, should_gate: true, trainable: false}
        fire8: {<<: *fire, should_gate: true, trainable: false}
        fire9: {<<: *fire, should_gate: true, trainable: false}
```

**sigyn**

* squeezenet_v11: gate all layers with 90%, started this with every layer set to be non-trainable. The previous run is saved as
`gate_offline_80_fire6_90_epoch.data-00000-of-00001`.
* The trick for offline to quickly reach its required gating density is to set `weight` to a relatively large value. However, a super-large weight can cause the gate loss to diverge.

```
LD_PRELOAD=/usr/lib/libtcmalloc.so ./my datasets/imagenet.yaml trainers/squeezenet.yaml models/gate/output/squeezenet_v11.yaml system.checkpoint.load=retrained system.num_gpus=2 system.visible_gpus=[0,1] system.batch_size_per_gpu=256 train.learning_rate._initial=0.001 system.preprocess.num_threads=32 train.learning_rate.decay_steps=20 reset-num-epochs train
```
