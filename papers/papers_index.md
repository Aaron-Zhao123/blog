---
layout: default
---

# [](#nn)Neural Networks
## [Network Models](/blog/papers/nn_models/model_summary.html)

A collection of popular neural network models: from the old LeNet model to ResNet model.
Some interesting network architectures are also collected.

## [Network Compression](/blog/papers/pruning/pruning_summary.html)
A range of compression techniques are summarized:
* Pruning
* Quantization
* Regularization
* Varying network structure

Pruning refers to move away connections in a neural network for compressing the size of it, both fine-grained and coarse-grained
pruning are discussed.
Vairous quantization methods: fixed-point, dynamic fixed-point, binarized, tenary.
Novel regularizers are considered as compression techniques.

## [Neural Network Accelerators](/blog/papers/nn_accelerator/acc_summary.html)
* Inference Accelerators -> aiming at low-power consumption
* Training Acceleraotors
* Frameworks
Accelerators for low-power systems, and accelerators in the cloud.

# [](#fpga)FPGAs
## [Applications](/blog/papers/fpga_nn/fpgann_summary.html)
My main focus on FPGAs in applications are still related to hardware architectures. This section specifically focus on the publications FPGA-based implementation of neural network accceleroatrs in the following conferences: FPGA, FCCM, FPL and FPT.

## [FPGAs in the cloud](/blog/papers/fpga_cloud/fpgncloud_summary.html)
The interest of FPGAs in the cloud origins from the Catapult project from Microsoft.

## [High level synthesis](/blog/papers/pruning/pruning_summary.html)
I have a particular interest in HLS. The popular HLS tools include the followings: Vivado HLS, Altera OpenCL and Legup.

# [](#others)Others
## [Scheduling Algorithms](/blog/papers/others/scheduling.html)
My research internship in Microsoft puts a focus on input-buffered switches in datacenters
